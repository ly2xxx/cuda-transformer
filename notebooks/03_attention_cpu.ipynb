{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Attention Mechanism (CPU Implementation)\n",
    "\n",
    "## Understanding the Heart of Transformers\n",
    "\n",
    "Welcome to the most important concept in modern NLP! In this notebook, you'll learn:\n",
    "\n",
    "1. **What is Attention?** - The intuition behind the mechanism\n",
    "2. **Query, Key, Value** - The three fundamental components\n",
    "3. **Scaled Dot-Product Attention** - The mathematical formula\n",
    "4. **Attention Visualization** - See what the model learns\n",
    "\n",
    "We'll start with CPU implementations to understand the concepts clearly before optimizing with GPU in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Intuition Behind Attention\n",
    "\n",
    "### The Problem: Context Matters\n",
    "\n",
    "Consider the sentence: **\"The animal didn't cross the street because it was too tired.\"**\n",
    "\n",
    "What does \"it\" refer to?\n",
    "- **The animal** (correct!)\n",
    "- The street (wrong)\n",
    "\n",
    "**Attention** lets the model focus on relevant words when processing each word.\n",
    "\n",
    "### The Solution: Weighted Representation\n",
    "\n",
    "Instead of treating all words equally, attention computes:\n",
    "- How **relevant** is each word to the current word?\n",
    "- Create a **weighted sum** of all words based on relevance\n",
    "\n",
    "### Analogy: Information Retrieval\n",
    "\n",
    "Think of attention like a search engine:\n",
    "1. **Query:** \"What are you looking for?\" (current word)\n",
    "2. **Keys:** \"What does each document contain?\" (all words)\n",
    "3. **Values:** \"The actual content\" (word representations)\n",
    "4. **Attention:** Match query to keys, retrieve weighted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Scaled Dot-Product Attention\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "Given:\n",
    "- $Q$ (Query): What we're looking for - shape $(n_{queries}, d_k)$\n",
    "- $K$ (Keys): What we're comparing against - shape $(n_{keys}, d_k)$\n",
    "- $V$ (Values): What we retrieve - shape $(n_{keys}, d_v)$\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Step by Step:\n",
    "\n",
    "1. **Compute Similarity:** $S = QK^T$ (how similar is each query to each key?)\n",
    "2. **Scale:** $S_{scaled} = S / \\sqrt{d_k}$ (prevent large values)\n",
    "3. **Normalize:** $A = \\text{softmax}(S_{scaled})$ (convert to probabilities)\n",
    "4. **Aggregate:** $\\text{Output} = AV$ (weighted sum of values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q: np.ndarray, \n",
    "                                  K: np.ndarray, \n",
    "                                  V: np.ndarray,\n",
    "                                  mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries of shape (n_queries, d_k)\n",
    "        K: Keys of shape (n_keys, d_k)\n",
    "        V: Values of shape (n_keys, d_v)\n",
    "        mask: Optional mask of shape (n_queries, n_keys)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output of shape (n_queries, d_v)\n",
    "        attention_weights: Attention weights of shape (n_queries, n_keys)\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Apply mask if provided (set masked positions to -inf)\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask, scores, -1e9)\n",
    "    \n",
    "    # Step 3: Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Step 4: Weighted sum of values\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "print(\"âœ… Attention function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example: Self-Attention on 3 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple example with 3 words, embedding dimension = 4\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# Input embeddings (random for demonstration)\n",
    "X = np.random.randn(seq_len, d_model).astype(np.float32)\n",
    "\n",
    "print(\"Input embeddings (3 words, 4 dimensions each):\")\n",
    "print(X)\n",
    "\n",
    "# For self-attention: Q = K = V = X\n",
    "output, attention_weights = scaled_dot_product_attention(X, X, X)\n",
    "\n",
    "print(\"\\nAttention Weights (how much each word attends to others):\")\n",
    "print(attention_weights)\n",
    "print(f\"\\nEach row sums to 1: {attention_weights.sum(axis=-1)}\")\n",
    "\n",
    "print(\"\\nOutput (context-aware representations):\")\n",
    "print(output)\n",
    "\n",
    "# Visualize attention\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attention_weights, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=[f'Word {i+1}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Word {i+1}' for i in range(seq_len)],\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Self-Attention Weights Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Keys (attending to)', fontsize=12)\n",
    "plt.ylabel('Queries (attending from)', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Each Component\n",
    "\n",
    "### What do Q, K, V actually do?\n",
    "\n",
    "In practice, Q, K, V are **learned projections** of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qkv_projections(d_model: int, d_k: int, d_v: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create random projection matrices (in real transformers, these are learned).\"\"\"\n",
    "    W_Q = np.random.randn(d_model, d_k).astype(np.float32) * 0.1\n",
    "    W_K = np.random.randn(d_model, d_k).astype(np.float32) * 0.1\n",
    "    W_V = np.random.randn(d_model, d_v).astype(np.float32) * 0.1\n",
    "    return W_Q, W_K, W_V\n",
    "\n",
    "# Create projection matrices\n",
    "d_model = 8  # Input embedding dimension\n",
    "d_k = 4      # Query/Key dimension\n",
    "d_v = 4      # Value dimension\n",
    "seq_len = 5  # Sequence length\n",
    "\n",
    "W_Q, W_K, W_V = create_qkv_projections(d_model, d_k, d_v)\n",
    "\n",
    "# Create input sequence\n",
    "X = np.random.randn(seq_len, d_model).astype(np.float32)\n",
    "\n",
    "# Project to Q, K, V\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Query shape: {Q.shape}\")\n",
    "print(f\"Key shape: {K.shape}\")\n",
    "print(f\"Value shape: {V.shape}\")\n",
    "\n",
    "# Apply attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Why Scaling by âˆšd_k?\n",
    "\n",
    "### The Problem: Dot Products Can Be Large\n",
    "\n",
    "When $d_k$ is large, dot products have large variance, pushing softmax into saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scaling(d_k_values: list) -> None:\n",
    "    \"\"\"Demonstrate the effect of scaling on attention.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(d_k_values), figsize=(15, 4))\n",
    "    \n",
    "    for idx, d_k in enumerate(d_k_values):\n",
    "        # Create random Q and K\n",
    "        Q = np.random.randn(5, d_k).astype(np.float32)\n",
    "        K = np.random.randn(5, d_k).astype(np.float32)\n",
    "        V = np.random.randn(5, d_k).astype(np.float32)\n",
    "        \n",
    "        # Without scaling\n",
    "        scores_no_scale = Q @ K.T\n",
    "        attn_no_scale = softmax(scores_no_scale, axis=-1)\n",
    "        \n",
    "        # With scaling\n",
    "        scores_scaled = (Q @ K.T) / np.sqrt(d_k)\n",
    "        attn_scaled = softmax(scores_scaled, axis=-1)\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[idx]\n",
    "        width = 0.35\n",
    "        x = np.arange(2)\n",
    "        \n",
    "        # Measure how \"peaked\" the distribution is (entropy)\n",
    "        entropy_no_scale = -np.sum(attn_no_scale * np.log(attn_no_scale + 1e-9))\n",
    "        entropy_scaled = -np.sum(attn_scaled * np.log(attn_scaled + 1e-9))\n",
    "        \n",
    "        ax.bar(x, [entropy_no_scale, entropy_scaled], width)\n",
    "        ax.set_ylabel('Entropy (higher = less peaked)', fontsize=10)\n",
    "        ax.set_title(f'd_k = {d_k}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(['No Scale', 'With Scale'])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Effect of Scaling on Attention Entropy', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ðŸ’¡ Key Insight: Scaling prevents attention from becoming too peaked\")\n",
    "    print(\"   Higher entropy = more distributed attention = better gradient flow\")\n",
    "\n",
    "compare_scaling([16, 64, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Masked Attention (Causal Attention)\n",
    "\n",
    "### The Need for Masking\n",
    "\n",
    "In language modeling, we predict the **next** word. The model shouldn't see future words!\n",
    "\n",
    "**Causal Mask:** Prevent position $i$ from attending to positions $j > i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int) -> np.ndarray:\n",
    "    \"\"\"Create lower-triangular causal mask.\"\"\"\n",
    "    mask = np.tril(np.ones((seq_len, seq_len), dtype=bool))\n",
    "    return mask\n",
    "\n",
    "# Example with sequence length 6\n",
    "seq_len = 6\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal Mask (1 = allowed, 0 = blocked):\")\n",
    "print(mask.astype(int))\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Show mask\n",
    "sns.heatmap(mask, annot=True, fmt='d', cmap='RdYlGn', \n",
    "            xticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            cbar=False, ax=ax1)\n",
    "ax1.set_title('Causal Mask', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Keys (past â†’ future)', fontsize=11)\n",
    "ax1.set_ylabel('Queries', fontsize=11)\n",
    "\n",
    "# Apply attention with mask\n",
    "X = np.random.randn(seq_len, 8).astype(np.float32)\n",
    "output, attn_weights = scaled_dot_product_attention(X, X, X, mask=mask)\n",
    "\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            xticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            ax=ax2)\n",
    "ax2.set_title('Masked Attention Weights', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Keys (past)', fontsize=11)\n",
    "ax2.set_ylabel('Queries', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Note: Each position only attends to itself and previous positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Real Example - Sentence Attention\n",
    "\n",
    "Let's see attention on an actual sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate word embeddings (in reality, these come from an embedding layer)\n",
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "vocab_size = 100\n",
    "d_model = 16\n",
    "\n",
    "# Random embeddings (pretend these are learned)\n",
    "word_to_idx = {word: i for i, word in enumerate(sentence)}\n",
    "embeddings = np.random.randn(len(sentence), d_model).astype(np.float32)\n",
    "\n",
    "# Create Q, K, V projections\n",
    "W_Q, W_K, W_V = create_qkv_projections(d_model, d_k=8, d_v=8)\n",
    "\n",
    "Q = embeddings @ W_Q\n",
    "K = embeddings @ W_K\n",
    "V = embeddings @ W_V\n",
    "\n",
    "# Compute attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# Visualize attention pattern\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.3f', cmap='viridis',\n",
    "            xticklabels=sentence, yticklabels=sentence,\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Self-Attention: \"The cat sat on the mat\"', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Keys (attending to)', fontsize=12)\n",
    "plt.ylabel('Queries (attending from)', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "for i, word in enumerate(sentence):\n",
    "    top_attn = np.argsort(attn_weights[i])[-3:][::-1]\n",
    "    print(f\"'{word}' pays most attention to: {[sentence[j] for j in top_attn]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Batched Attention\n",
    "\n",
    "In practice, we process multiple sequences simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Batched scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: (batch, n_queries, d_k)\n",
    "        K: (batch, n_keys, d_k)\n",
    "        V: (batch, n_keys, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, n_queries, d_v)\n",
    "        attention_weights: (batch, n_queries, n_keys)\n",
    "    \"\"\"\n",
    "    batch_size = Q.shape[0]\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Batched matrix multiplication: (batch, n_q, d_k) @ (batch, d_k, n_k)\n",
    "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # Softmax over last dimension\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Weighted sum: (batch, n_q, n_k) @ (batch, n_k, d_v)\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test batched attention\n",
    "batch_size = 8\n",
    "seq_len = 10\n",
    "d_model = 16\n",
    "\n",
    "Q = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)\n",
    "K = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)\n",
    "V = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)\n",
    "\n",
    "output, attn = batched_attention(Q, K, V)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"\\nInput Q shape: {Q.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention shape: {attn.shape}\")\n",
    "print(f\"\\nAttention weights sum per query: {attn[0, 0].sum():.6f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section\n",
    "\n",
    "### Exercise 1: Cross-Attention\n",
    "Implement cross-attention where queries come from one sequence and keys/values from another:\n",
    "- Useful in encoder-decoder architectures\n",
    "- Decoder attends to encoder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement cross-attention\n",
    "def cross_attention(Q_decoder, K_encoder, V_encoder):\n",
    "    \"\"\"\n",
    "    Implement cross-attention between decoder and encoder.\n",
    "    Q comes from decoder, K and V from encoder.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Attention Pattern Analysis\n",
    "Create different attention patterns and analyze their properties:\n",
    "1. Uniform attention (all weights equal)\n",
    "2. Peaked attention (one dominant weight)\n",
    "3. Local attention (nearby positions)\n",
    "\n",
    "Compute entropy for each and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze different attention patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Attention Dropout\n",
    "Implement attention with dropout:\n",
    "- Randomly zero out some attention weights\n",
    "- Helps prevent overfitting\n",
    "- Compare with and without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement attention with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "âœ… **Attention Mechanism:**\n",
    "- Computes weighted combinations based on relevance\n",
    "- Query-Key similarity determines weights\n",
    "- Values are aggregated using these weights\n",
    "\n",
    "âœ… **Components:**\n",
    "- **Q (Query):** What are we looking for?\n",
    "- **K (Keys):** What do we have?\n",
    "- **V (Values):** What information to retrieve?\n",
    "\n",
    "âœ… **Important Details:**\n",
    "- Scaling by âˆšd_k prevents saturation\n",
    "- Masking enables causal (autoregressive) attention\n",
    "- Softmax ensures weights sum to 1\n",
    "\n",
    "âœ… **Applications:**\n",
    "- Self-attention: relate positions within sequence\n",
    "- Cross-attention: relate two different sequences\n",
    "- Causal attention: for language modeling\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Notebook 04**, we'll:\n",
    "- Implement attention on GPU with PyTorch\n",
    "- Benchmark CPU vs GPU performance\n",
    "- Optimize for large-scale processing\n",
    "- Handle batching efficiently\n",
    "\n",
    "## Further reading (Archive.org)\n",
    "\n",
    "For conceptual background on attention mechanisms and sequence models, try Archive.org searches such as:\n",
    "\n",
    "- \"neural networks attention\"\n",
    "- \"sequence to sequence models\"\n",
    "- \"deep learning attention tutorial\"\n",
    "\n",
    "Combine these with modern Transformer introductions (for example, general surveys of Transformer models) to reinforce your understanding of queries, keys, values, and the scaled dot-product attention formula implemented in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

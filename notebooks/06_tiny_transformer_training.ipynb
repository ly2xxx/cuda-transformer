{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06: Training a Tiny Transformer\n",
    "\n",
    "## From Theory to Practice - Building a Language Model\n",
    "\n",
    "Congratulations on making it to the final notebook! Now we'll bring everything together and train a working transformer. In this notebook:\n",
    "\n",
    "1. **Character-Level Language Modeling** - Predicting next characters\n",
    "2. **Training Loop** - Complete training pipeline\n",
    "3. **Text Generation** - Sampling from the trained model\n",
    "4. **Training Dynamics** - Monitoring loss, gradients, attention\n",
    "\n",
    "By the end, you'll have a trained transformer that generates text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dataset Preparation\n",
    "\n",
    "We'll train on a small text corpus (Shakespeare, for tradition!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text (replace with actual data in practice)\n",
    "SAMPLE_TEXT = \"\"\"\n",
    "To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them. To dieâ€”to sleep,\n",
    "No more; and by a sleep to say we end\n",
    "The heart-ache and the thousand natural shocks\n",
    "That flesh is heir to: 'tis a consummation\n",
    "Devoutly to be wish'd. To die, to sleep;\n",
    "To sleep, perchance to dreamâ€”ay, there's the rub:\n",
    "For in that sleep of death what dreams may come,\n",
    "When we have shuffled off this mortal coil,\n",
    "Must give us pauseâ€”there's the respect\n",
    "That makes calamity of so long life.\n",
    "\"\"\"\n",
    "\n",
    "# For real training, download Shakespeare corpus:\n",
    "try:\n",
    "    import urllib.request\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text = response.read().decode('utf-8')\n",
    "    print(f\"âœ… Downloaded Shakespeare corpus: {len(text):,} characters\")\n",
    "except:\n",
    "    text = SAMPLE_TEXT * 100  # Repeat sample if download fails\n",
    "    print(f\"âš ï¸ Using sample text: {len(text):,} characters\")\n",
    "\n",
    "# Create character vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars[:50])}...\")\n",
    "\n",
    "# Create mappings\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode/decode functions\n",
    "def encode(text: str) -> list:\n",
    "    return [char_to_idx[ch] for ch in text]\n",
    "\n",
    "def decode(indices: list) -> str:\n",
    "    return ''.join([idx_to_char[i] for i in indices])\n",
    "\n",
    "# Test encoding\n",
    "sample = \"Hello\"\n",
    "encoded = encode(sample)\n",
    "decoded = decode(encoded)\n",
    "print(f\"\\nTest: '{sample}' â†’ {encoded} â†’ '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"Character-level dataset for next-token prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, text: str, seq_len: int, char_to_idx: dict):\n",
    "        self.seq_len = seq_len\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.data = [char_to_idx[ch] for ch in text]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: characters [idx:idx+seq_len]\n",
    "        # Target: characters [idx+1:idx+seq_len+1] (shifted by 1)\n",
    "        x = torch.tensor(self.data[idx:idx + self.seq_len], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx + 1:idx + self.seq_len + 1], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloader\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "\n",
    "dataset = CharDataset(text, seq_len, char_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset):,} sequences\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Show a sample\n",
    "x, y = next(iter(dataloader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Target shape: {y.shape}\")\n",
    "print(f\"\\nFirst sequence:\")\n",
    "print(f\"  Input:  '{decode(x[0].tolist())}'\")\n",
    "print(f\"  Target: '{decode(y[0].tolist())}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Definition\n",
    "\n",
    "Let's create a small transformer for character-level modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our transformer components from previous notebook\n",
    "# (In practice, these would be in a separate module)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        Q = self.W_Q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.W_O(attn_output)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual\n",
    "        attn_out = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "class CharTransformer(nn.Module):\n",
    "    \"\"\"Character-level transformer language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, \n",
    "                 max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
    "        x = self.token_embedding(x) + self.pos_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).view(1, 1, seq_len, seq_len)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "# Create model\n",
    "model = CharTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    d_ff=512,\n",
    "    max_len=seq_len,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"âœ… Model created\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training Loop\n",
    "\n",
    "Let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "learning_rate = 3e-4\n",
    "max_epochs = 10\n",
    "eval_interval = 100\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_epochs * len(dataloader))\n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "step = 0\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{max_epochs}\")\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(progress_bar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(x, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        train_losses.append(loss.item())\n",
    "        step += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}: Average Loss = {avg_loss:.4f}\\n\")\n",
    "\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, alpha=0.3)\n",
    "# Smooth with moving average\n",
    "window = 50\n",
    "if len(train_losses) > window:\n",
    "    smoothed = np.convolve(train_losses, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(train_losses)), smoothed, linewidth=2, label='Smoothed')\n",
    "plt.xlabel('Step', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses[-1000:])  # Last 1000 steps\n",
    "plt.xlabel('Recent Steps', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Recent Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best loss: {min(train_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Text Generation\n",
    "\n",
    "Now for the fun part - generating text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, prompt, max_new_tokens=100, temperature=0.8, top_k=40):\n",
    "    \"\"\"\n",
    "    Generate text from the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained transformer\n",
    "        prompt: Starting text\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: Sample from top-k most likely tokens\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    generated = prompt\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop context if needed\n",
    "        context_cond = context if context.size(1) <= seq_len else context[:, -seq_len:]\n",
    "        \n",
    "        # Get predictions\n",
    "        logits, _ = model(context_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Top-k sampling\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        \n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append to sequence\n",
    "        context = torch.cat([context, next_token], dim=1)\n",
    "        generated += idx_to_char[next_token.item()]\n",
    "    \n",
    "    model.train()\n",
    "    return generated\n",
    "\n",
    "# Generate samples with different prompts and temperatures\n",
    "prompts = [\"To be\", \"The \", \"What \"]\n",
    "temperatures = [0.5, 0.8, 1.0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Generated Samples\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature = {temp}\")\n",
    "    print(\"-\"*80)\n",
    "    for prompt in prompts:\n",
    "        generated = generate(model, prompt, max_new_tokens=200, temperature=temp, top_k=40)\n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        print(f\"Generated: {generated}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Analysis\n",
    "\n",
    "Let's analyze what the model learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity on validation set\n",
    "@torch.no_grad()\n",
    "def compute_perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        total_loss += loss.item() * x.size(0) * x.size(1)\n",
    "        total_tokens += x.size(0) * x.size(1)\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    model.train()\n",
    "    return perplexity, avg_loss\n",
    "\n",
    "perplexity, avg_loss = compute_perplexity(model, dataloader)\n",
    "print(f\"Model Perplexity: {perplexity:.2f}\")\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"\\nðŸ’¡ Lower perplexity = better model\")\n",
    "print(f\"   Random baseline would have perplexity â‰ˆ {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "@torch.no_grad()\n",
    "def visualize_attention(model, text, layer_idx=0, head_idx=0):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode text\n",
    "    tokens = torch.tensor(encode(text), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Get embeddings\n",
    "    positions = torch.arange(0, tokens.size(1), device=device).unsqueeze(0)\n",
    "    x = model.token_embedding(tokens) + model.pos_embedding(positions)\n",
    "    \n",
    "    # Get attention from specific layer and head\n",
    "    mask = torch.tril(torch.ones(tokens.size(1), tokens.size(1), device=device)).view(1, 1, -1, tokens.size(1))\n",
    "    \n",
    "    for i, block in enumerate(model.blocks):\n",
    "        if i == layer_idx:\n",
    "            # Extract attention weights\n",
    "            Q = block.attention.W_Q(x).view(1, -1, block.attention.n_heads, block.attention.d_k).transpose(1, 2)\n",
    "            K = block.attention.W_K(x).view(1, -1, block.attention.n_heads, block.attention.d_k).transpose(1, 2)\n",
    "            \n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(block.attention.d_k)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            \n",
    "            attn_matrix = attn_weights[0, head_idx].cpu().numpy()\n",
    "            break\n",
    "        x = block(x, mask)\n",
    "    \n",
    "    # Visualize\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attn_matrix, cmap='viridis', xticklabels=list(text), yticklabels=list(text))\n",
    "    plt.title(f'Attention Pattern (Layer {layer_idx}, Head {head_idx})', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Keys', fontsize=12)\n",
    "    plt.ylabel('Queries', fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Visualize attention on sample text\n",
    "sample_text = \"To be or not to be\"\n",
    "visualize_attention(model, sample_text, layer_idx=0, head_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'char_to_idx': char_to_idx,\n",
    "    'idx_to_char': idx_to_char,\n",
    "    'config': {\n",
    "        'd_model': 128,\n",
    "        'n_layers': 4,\n",
    "        'n_heads': 4,\n",
    "        'd_ff': 512,\n",
    "        'max_len': seq_len\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'char_transformer.pt')\n",
    "print(\"âœ… Model saved to 'char_transformer.pt'\")\n",
    "\n",
    "# Load model (example)\n",
    "# checkpoint = torch.load('char_transformer.pt')\n",
    "# model = CharTransformer(**checkpoint['config'], vocab_size=checkpoint['vocab_size'])\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section\n",
    "\n",
    "### Exercise 1: Experiment with Architectures\n",
    "Train models with different configurations:\n",
    "- Vary number of layers (2, 4, 8)\n",
    "- Vary d_model (64, 128, 256)\n",
    "- Vary number of heads\n",
    "\n",
    "Compare perplexity and generation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Beam Search\n",
    "Instead of sampling, implement beam search:\n",
    "- Keep top-k sequences at each step\n",
    "- Score by log probability\n",
    "- Compare with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Analyze Learned Patterns\n",
    "Investigate what the model learned:\n",
    "- Which characters predict which?\n",
    "- Attention to punctuation\n",
    "- Long-range dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze learned patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: You've Built a Transformer!\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "âœ… **End-to-End Training:**\n",
    "- Dataset preparation and batching\n",
    "- Complete training loop with optimization\n",
    "- Text generation and evaluation\n",
    "\n",
    "âœ… **Model Architecture:**\n",
    "- Multi-head self-attention\n",
    "- Position-wise feed-forward\n",
    "- Layer normalization and residuals\n",
    "- Positional embeddings\n",
    "\n",
    "âœ… **Training Techniques:**\n",
    "- AdamW optimizer\n",
    "- Learning rate scheduling\n",
    "- Gradient clipping\n",
    "- Dropout regularization\n",
    "\n",
    "âœ… **Generation:**\n",
    "- Temperature sampling\n",
    "- Top-k filtering\n",
    "- Autoregressive generation\n",
    "\n",
    "### Journey Recap\n",
    "\n",
    "From Notebook 01 to now:\n",
    "1. **CUDA Basics** - GPU programming fundamentals\n",
    "2. **Matrix Ops & Softmax** - Neural network primitives\n",
    "3. **Attention (CPU)** - Understanding the mechanism\n",
    "4. **Attention (GPU)** - Scaling with hardware\n",
    "5. **Transformer Block** - Complete architecture\n",
    "6. **Training** - Working language model!\n",
    "\n",
    "### Next Steps in Your Journey\n",
    "\n",
    "**Immediate:**\n",
    "- Train on larger datasets (full Shakespeare, Wikipedia)\n",
    "- Experiment with word-level or BPE tokenization\n",
    "- Try different architectures (GPT-style decoder-only)\n",
    "\n",
    "**Intermediate:**\n",
    "- Implement Flash Attention for memory efficiency\n",
    "- Add more sophisticated sampling (nucleus, typical)\n",
    "- Fine-tune pre-trained models\n",
    "\n",
    "**Advanced:**\n",
    "- Multi-GPU training with DDP\n",
    "- Model quantization and optimization\n",
    "- Custom CUDA kernels for specific operations\n",
    "\n",
    "### Resources for Continued Learning\n",
    "\n",
    "**Papers:**\n",
    "- \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
    "- \"BERT\" (Devlin et al., 2018)\n",
    "- \"GPT-3\" (Brown et al., 2020)\n",
    "- \"Flash Attention\" (Dao et al., 2022)\n",
    "\n",
    "**Code:**\n",
    "- HuggingFace Transformers library\n",
    "- Andrej Karpathy's nanoGPT\n",
    "- Fairseq (Facebook AI)\n",
    "\n",
    "## Further reading (Archive.org)\n",
    "\n",
    "To place this tiny Transformer training loop in context, use Archive.org searches such as:\n",
    "\n",
    "- \"sequence to sequence learning tutorial\"\n",
    "- \"language modeling deep learning\"\n",
    "- \"character level language model\"\n",
    "\n",
    "Prioritize lecture notes or book chapters that describe next-token prediction, cross-entropy loss, and teacher forcing, as they closely match the toy training setup and sampling procedure implemented here.\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've completed the CUDA Transformer tutorial. You now understand:\n",
    "- How GPUs accelerate deep learning\n",
    "- The mathematics behind transformers\n",
    "- How to implement and train modern architectures\n",
    "\n",
    "**Keep building, keep learning, and enjoy your journey in deep learning!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: GPU-Accelerated Attention\n",
    "\n",
    "## Scaling Attention with CUDA\n",
    "\n",
    "Now that you understand attention conceptually, let's implement it efficiently on GPU! In this notebook:\n",
    "\n",
    "1. **PyTorch GPU Implementation** - Converting CPU code to GPU\n",
    "2. **Performance Benchmarking** - CPU vs GPU speedup\n",
    "3. **Memory Optimization** - Managing large attention matrices\n",
    "4. **Batched Processing** - Handling multiple sequences efficiently\n",
    "\n",
    "This is where transformers truly shine - parallel attention across massive datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available. Using CPU (will be slow).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: GPU Attention Implementation\n",
    "\n",
    "### PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_gpu(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    GPU-accelerated scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries (batch, n_heads, seq_len, d_k)\n",
    "        K: Keys (batch, n_heads, seq_len, d_k)\n",
    "        V: Values (batch, n_heads, seq_len, d_v)\n",
    "        mask: Optional mask (batch, n_heads, seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (batch, n_heads, seq_len, d_v)\n",
    "        attention_weights: (batch, n_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32, device=Q.device))\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test the implementation\n",
    "batch_size = 4\n",
    "n_heads = 8\n",
    "seq_len = 16\n",
    "d_k = 64\n",
    "\n",
    "Q = torch.randn(batch_size, n_heads, seq_len, d_k, device=device)\n",
    "K = torch.randn(batch_size, n_heads, seq_len, d_k, device=device)\n",
    "V = torch.randn(batch_size, n_heads, seq_len, d_k, device=device)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention_gpu(Q, K, V)\n",
    "\n",
    "print(f\"‚úÖ GPU Attention Implementation\")\n",
    "print(f\"Input Q shape: {Q.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention weights sum check: {attn_weights.sum(dim=-1)[0, 0, 0]:.6f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Performance Comparison - CPU vs GPU\n",
    "\n",
    "Let's benchmark the performance difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention(seq_lengths: list, d_k: int = 64, batch_size: int = 32) -> dict:\n",
    "    \"\"\"Benchmark attention performance across different sequence lengths.\"\"\"\n",
    "    cpu_times = []\n",
    "    gpu_times = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        print(f\"Benchmarking seq_len={seq_len}...\")\n",
    "        \n",
    "        # Create test data\n",
    "        Q_cpu = torch.randn(batch_size, 1, seq_len, d_k)\n",
    "        K_cpu = torch.randn(batch_size, 1, seq_len, d_k)\n",
    "        V_cpu = torch.randn(batch_size, 1, seq_len, d_k)\n",
    "        \n",
    "        # CPU benchmark\n",
    "        start = time.time()\n",
    "        _, _ = scaled_dot_product_attention_gpu(Q_cpu, K_cpu, V_cpu)\n",
    "        cpu_times.append((time.time() - start) * 1000)\n",
    "        \n",
    "        # GPU benchmark\n",
    "        if torch.cuda.is_available():\n",
    "            Q_gpu = Q_cpu.to(device)\n",
    "            K_gpu = K_cpu.to(device)\n",
    "            V_gpu = V_cpu.to(device)\n",
    "            \n",
    "            # Warmup\n",
    "            _, _ = scaled_dot_product_attention_gpu(Q_gpu, K_gpu, V_gpu)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Actual benchmark\n",
    "            start = time.time()\n",
    "            _, _ = scaled_dot_product_attention_gpu(Q_gpu, K_gpu, V_gpu)\n",
    "            torch.cuda.synchronize()\n",
    "            gpu_times.append((time.time() - start) * 1000)\n",
    "        else:\n",
    "            gpu_times.append(0)\n",
    "    \n",
    "    return {'cpu': cpu_times, 'gpu': gpu_times}\n",
    "\n",
    "seq_lengths = [32, 64, 128, 256, 512]\n",
    "results = benchmark_attention(seq_lengths)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Attention Performance Benchmark\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Seq Length':<12} {'CPU (ms)':<15} {'GPU (ms)':<15} {'Speedup':<12}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for seq_len, cpu_t, gpu_t in zip(seq_lengths, results['cpu'], results['gpu']):\n",
    "    speedup = cpu_t / gpu_t if gpu_t > 0 else 0\n",
    "    print(f\"{seq_len:<12} {cpu_t:<15.2f} {gpu_t:<15.4f} {speedup:<12.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance\n",
    "if torch.cuda.is_available():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Time comparison\n",
    "    ax1.semilogy(seq_lengths, results['cpu'], 'o-', label='CPU', linewidth=2, markersize=8)\n",
    "    ax1.semilogy(seq_lengths, results['gpu'], 's-', label='GPU', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Sequence Length', fontsize=12)\n",
    "    ax1.set_ylabel('Time (ms, log scale)', fontsize=12)\n",
    "    ax1.set_title('Attention Performance: CPU vs GPU', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Speedup\n",
    "    speedups = [cpu / gpu for cpu, gpu in zip(results['cpu'], results['gpu'])]\n",
    "    ax2.plot(seq_lengths, speedups, 'o-', linewidth=2, markersize=8, color='green')\n",
    "    ax2.set_xlabel('Sequence Length', fontsize=12)\n",
    "    ax2.set_ylabel('Speedup (√ó)', fontsize=12)\n",
    "    ax2.set_title('GPU Speedup over CPU', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=1, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Maximum speedup: {max(speedups):.1f}x at seq_len={seq_lengths[speedups.index(max(speedups))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Memory Considerations\n",
    "\n",
    "### The Memory Challenge\n",
    "\n",
    "Attention requires $O(n^2)$ memory for the attention matrix, where $n$ is sequence length.\n",
    "\n",
    "**Example:** For seq_len=1024, batch=32, heads=8:\n",
    "- Attention matrix: `32 √ó 8 √ó 1024 √ó 1024 √ó 4 bytes = 1 GB!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_attention_memory(batch_size: int, n_heads: int, seq_len: int, d_k: int) -> dict:\n",
    "    \"\"\"Estimate memory requirements for attention.\"\"\"\n",
    "    # All values in bytes (float32 = 4 bytes)\n",
    "    bytes_per_element = 4\n",
    "    \n",
    "    # Q, K, V storage\n",
    "    qkv_memory = 3 * batch_size * n_heads * seq_len * d_k * bytes_per_element\n",
    "    \n",
    "    # Attention scores matrix\n",
    "    scores_memory = batch_size * n_heads * seq_len * seq_len * bytes_per_element\n",
    "    \n",
    "    # Output\n",
    "    output_memory = batch_size * n_heads * seq_len * d_k * bytes_per_element\n",
    "    \n",
    "    total_memory = qkv_memory + scores_memory + output_memory\n",
    "    \n",
    "    return {\n",
    "        'QKV (MB)': qkv_memory / 1e6,\n",
    "        'Scores (MB)': scores_memory / 1e6,\n",
    "        'Output (MB)': output_memory / 1e6,\n",
    "        'Total (MB)': total_memory / 1e6,\n",
    "        'Total (GB)': total_memory / 1e9\n",
    "    }\n",
    "\n",
    "# Test different configurations\n",
    "configs = [\n",
    "    (32, 8, 512, 64),\n",
    "    (32, 8, 1024, 64),\n",
    "    (32, 8, 2048, 64),\n",
    "    (64, 16, 1024, 64)\n",
    "]\n",
    "\n",
    "print(\"Memory Requirements for Different Configurations:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Config':<25} {'QKV (MB)':<12} {'Scores (MB)':<15} {'Total (MB)':<12}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for batch, heads, seq, d_k in configs:\n",
    "    mem = estimate_attention_memory(batch, heads, seq, d_k)\n",
    "    config_str = f\"B={batch}, H={heads}, L={seq}\"\n",
    "    print(f\"{config_str:<25} {mem['QKV (MB)']:<12.1f} {mem['Scores (MB)']:<15.1f} {mem['Total (MB)']:<12.1f}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Attention scores dominate memory for large sequences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Head Attention\n",
    "\n",
    "### Why Multiple Heads?\n",
    "\n",
    "Multi-head attention allows the model to attend to different aspects simultaneously:\n",
    "- Head 1: Syntactic relationships\n",
    "- Head 2: Semantic meaning\n",
    "- Head 3: Long-range dependencies\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_O = torch.nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, \n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Linear projections and reshape to (batch, n_heads, seq_len, d_k)\n",
    "        Q = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, _ = scaled_dot_product_attention_gpu(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads: (batch, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_O(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "batch_size = 16\n",
    "seq_len = 32\n",
    "\n",
    "mha = MultiHeadAttention(d_model, n_heads).to(device)\n",
    "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "\n",
    "output = mha(x, x, x)\n",
    "\n",
    "print(f\"‚úÖ Multi-Head Attention\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights from individual heads\n",
    "with torch.no_grad():\n",
    "    x_small = torch.randn(1, 10, d_model, device=device)\n",
    "    \n",
    "    Q = mha.W_Q(x_small).view(1, 10, n_heads, mha.d_k).transpose(1, 2)\n",
    "    K = mha.W_K(x_small).view(1, 10, n_heads, mha.d_k).transpose(1, 2)\n",
    "    V = mha.W_V(x_small).view(1, 10, n_heads, mha.d_k).transpose(1, 2)\n",
    "    \n",
    "    _, attn_weights = scaled_dot_product_attention_gpu(Q, K, V)\n",
    "    attn_weights = attn_weights.cpu().numpy()[0]  # (n_heads, seq_len, seq_len)\n",
    "\n",
    "# Plot first 4 attention heads\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(4):\n",
    "    sns.heatmap(attn_weights[i], annot=True, fmt='.2f', cmap='viridis',\n",
    "                ax=axes[i], cbar_kws={'label': 'Attention Weight'})\n",
    "    axes[i].set_title(f'Attention Head {i+1}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Key Position')\n",
    "    axes[i].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice: Different heads learn different attention patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Optimizations and Best Practices\n",
    "\n",
    "### 1. Gradient Checkpointing (Trading Compute for Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage with/without gradient checkpointing\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "def measure_memory_usage(model, x, use_checkpoint=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    if use_checkpoint:\n",
    "        output = checkpoint(model, x, x, x)\n",
    "    else:\n",
    "        output = model(x, x, x)\n",
    "    \n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e6\n",
    "    return peak_memory\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    mha = MultiHeadAttention(512, 8).to(device)\n",
    "    x = torch.randn(8, 64, 512, device=device, requires_grad=True)\n",
    "    \n",
    "    mem_normal = measure_memory_usage(mha, x, use_checkpoint=False)\n",
    "    mem_checkpoint = measure_memory_usage(mha, x, use_checkpoint=True)\n",
    "    \n",
    "    print(f\"Memory Usage Comparison:\")\n",
    "    print(f\"  Normal: {mem_normal:.1f} MB\")\n",
    "    print(f\"  With Checkpointing: {mem_checkpoint:.1f} MB\")\n",
    "    print(f\"  Savings: {(1 - mem_checkpoint/mem_normal) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Flash Attention (Modern Optimization)\n",
    "\n",
    "PyTorch 2.0+ includes optimized attention implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyTorch's optimized scaled_dot_product_attention (if available)\n",
    "if hasattr(F, 'scaled_dot_product_attention'):\n",
    "    def fast_attention(Q, K, V, mask=None):\n",
    "        \"\"\"Use PyTorch's optimized implementation.\"\"\"\n",
    "        return F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)\n",
    "    \n",
    "    print(\"‚úÖ PyTorch optimized attention available!\")\n",
    "    print(\"   This includes Flash Attention optimizations\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using manual implementation (PyTorch < 2.0)\")\n",
    "    print(\"   Consider upgrading for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section\n",
    "\n",
    "### Exercise 1: Attention Dropout\n",
    "Add dropout to attention weights:\n",
    "- Apply after softmax\n",
    "- Compare training stability with/without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement attention with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Cross-Attention GPU\n",
    "Implement cross-attention on GPU:\n",
    "- Different Q vs K, V sequences\n",
    "- Benchmark performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Memory Profiling\n",
    "Profile memory usage for different configurations:\n",
    "- Vary batch size, sequence length, number of heads\n",
    "- Find maximum feasible configuration for your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Profile memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **GPU Acceleration:**\n",
    "- PyTorch handles CUDA operations automatically\n",
    "- Massive speedup for large sequences (50-100x+)\n",
    "- Critical for training large transformers\n",
    "\n",
    "‚úÖ **Memory Management:**\n",
    "- Attention has $O(n^2)$ memory complexity\n",
    "- Gradient checkpointing trades compute for memory\n",
    "- Monitor GPU memory usage carefully\n",
    "\n",
    "‚úÖ **Multi-Head Attention:**\n",
    "- Parallel attention over different representations\n",
    "- Each head learns unique patterns\n",
    "- Concatenate and project back to d_model\n",
    "\n",
    "‚úÖ **Optimization:**\n",
    "- Use PyTorch's optimized implementations when available\n",
    "- Flash Attention for better memory/speed\n",
    "- Batch operations for efficiency\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Notebook 05**, we'll build:\n",
    "- Complete transformer encoder block\n",
    "- Feed-forward networks\n",
    "- Layer normalization and residual connections\n",
    "- Full encoder stack\n",
    "\n",
    "## Further reading (Archive.org)\n",
    "\n",
    "To connect attention mechanisms with GPU implementation details, search Archive.org for:\n",
    "\n",
    "- \"GPU deep learning\"\n",
    "- \"high performance deep learning\"\n",
    "- \"CUDA deep learning kernels\"\n",
    "\n",
    "Look for discussions of memory access patterns, kernel fusion, and batching strategies, which will help you reason about how an attention kernel can be optimized beyond the straightforward PyTorch implementation used here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: CUDA Matrix Operations & Softmax\n",
    "\n",
    "## Building Neural Network Primitives on GPU\n",
    "\n",
    "Welcome to Notebook 02! Now that you understand CUDA basics, we'll implement essential neural network operations:\n",
    "\n",
    "1. **Optimized Matrix Multiplication** - Using shared memory and tiling\n",
    "2. **Stable Softmax** - Critical for attention mechanisms\n",
    "3. **Memory Coalescing** - Maximizing bandwidth\n",
    "4. **Performance Optimization** - Profiling and tuning\n",
    "\n",
    "These operations form the foundation of transformer models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Matrix Multiplication Optimization\n",
    "\n",
    "### Understanding Shared Memory\n",
    "\n",
    "**Memory Hierarchy Speed:**\n",
    "```\n",
    "Registers:      ~1 cycle     (fastest, tiny)\n",
    "Shared Memory:  ~5 cycles    (fast, small - 48KB per block)\n",
    "Global Memory:  ~400 cycles  (slow, large - GBs)\n",
    "```\n",
    "\n",
    "**Tiling Strategy:**\n",
    "Instead of reading from global memory for every computation:\n",
    "1. Load tiles into shared memory\n",
    "2. Compute using fast shared memory\n",
    "3. Repeat for all tiles\n",
    "\n",
    "### Naive vs Tiled Matrix Multiplication\n",
    "\n",
    "![Matrix Multiplication Tiling](https://via.placeholder.com/600x300.png?text=Tiling+Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive_gpu(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Naive matrix multiplication - each element computed independently.\"\"\"\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "# PyTorch's optimized version uses tiling internally\n",
    "# Let's compare with explicit loops to understand the difference\n",
    "\n",
    "def matmul_explicit_loops(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Explicit triple-loop implementation (SLOW - for demonstration).\"\"\"\n",
    "    m, k = A.shape\n",
    "    k2, n = B.shape\n",
    "    assert k == k2\n",
    "    \n",
    "    C = np.zeros((m, n), dtype=np.float32)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            for k_idx in range(k):\n",
    "                C[i, j] += A[i, k_idx] * B[k_idx, j]\n",
    "    return C\n",
    "\n",
    "# Test\n",
    "size = 256\n",
    "A = torch.randn(size, size, device=device)\n",
    "B = torch.randn(size, size, device=device)\n",
    "\n",
    "# Warmup\n",
    "_ = torch.matmul(A, B)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Benchmark optimized version\n",
    "start = time.time()\n",
    "C = torch.matmul(A, B)\n",
    "torch.cuda.synchronize()\n",
    "gpu_time = (time.time() - start) * 1000\n",
    "\n",
    "print(f\"Optimized GPU MatMul ({size}Ã—{size}): {gpu_time:.2f} ms\")\n",
    "print(f\"\\nThroughput: {(2 * size**3) / (gpu_time / 1000) / 1e9:.2f} GFLOPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Memory Access Patterns\n",
    "\n",
    "Understanding memory coalescing is crucial for GPU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_memory_patterns(sizes: list) -> dict:\n",
    "    \"\"\"Compare different memory access patterns.\"\"\"\n",
    "    results = {'contiguous': [], 'transpose': []}\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Contiguous access (fast)\n",
    "        A = torch.randn(size, size, device=device)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        _ = A.sum(dim=1)  # Row-wise (contiguous in memory)\n",
    "        torch.cuda.synchronize()\n",
    "        results['contiguous'].append((time.time() - start) * 1000)\n",
    "        \n",
    "        # Non-contiguous access (slower)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        _ = A.sum(dim=0)  # Column-wise (strided access)\n",
    "        torch.cuda.synchronize()\n",
    "        results['transpose'].append((time.time() - start) * 1000)\n",
    "    \n",
    "    return results\n",
    "\n",
    "sizes = [512, 1024, 2048, 4096]\n",
    "mem_results = benchmark_memory_patterns(sizes)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sizes, mem_results['contiguous'], 'o-', label='Row-wise (coalesced)', linewidth=2)\n",
    "plt.plot(sizes, mem_results['transpose'], 's-', label='Column-wise (strided)', linewidth=2)\n",
    "plt.xlabel('Matrix Size', fontsize=12)\n",
    "plt.ylabel('Time (ms)', fontsize=12)\n",
    "plt.title('Memory Access Patterns Impact', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: Contiguous memory access is faster!\")\n",
    "print(\"   GPUs read memory in coalesced chunks (32/128 bytes)\")\n",
    "print(\"   Arrange data for sequential access when possible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Softmax - The Foundation of Attention\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For a vector $x = [x_1, x_2, ..., x_n]$:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
    "\n",
    "### Numerical Stability Challenge\n",
    "\n",
    "**Problem:** Large values cause overflow:\n",
    "- $e^{1000}$ = overflow!\n",
    "- $e^{-1000}$ = underflow to 0\n",
    "\n",
    "**Solution:** Subtract max before exponential:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_{j=1}^{n} e^{x_j - \\max(x)}}$$\n",
    "\n",
    "This is mathematically equivalent but numerically stable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Naive softmax - UNSTABLE for large values.\"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def softmax_stable(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    x_max = np.max(x)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Demonstrate stability issue\n",
    "x_small = np.array([1.0, 2.0, 3.0], dtype=np.float32)\n",
    "x_large = np.array([1000.0, 1001.0, 1002.0], dtype=np.float32)\n",
    "\n",
    "print(\"Small values:\")\n",
    "print(f\"  Naive:  {softmax_naive(x_small)}\")\n",
    "print(f\"  Stable: {softmax_stable(x_small)}\")\n",
    "print(f\"  Match: {np.allclose(softmax_naive(x_small), softmax_stable(x_small))}\")\n",
    "\n",
    "print(\"\\nLarge values:\")\n",
    "try:\n",
    "    naive_result = softmax_naive(x_large)\n",
    "    print(f\"  Naive:  {naive_result}\")\n",
    "    if np.any(np.isnan(naive_result)):\n",
    "        print(\"  âš ï¸ NaN detected - numerical overflow!\")\n",
    "except:\n",
    "    print(\"  âš ï¸ Naive version FAILED - overflow!\")\n",
    "\n",
    "stable_result = softmax_stable(x_large)\n",
    "print(f\"  Stable: {stable_result}\")\n",
    "print(f\"  Sum:    {np.sum(stable_result):.6f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Softmax Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_gpu(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    \"\"\"Numerically stable GPU softmax.\"\"\"\n",
    "    return torch.softmax(x, dim=dim)\n",
    "\n",
    "def softmax_manual(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    \"\"\"Manual stable softmax implementation.\"\"\"\n",
    "    x_max = x.max(dim=dim, keepdim=True)[0]\n",
    "    exp_x = torch.exp(x - x_max)\n",
    "    return exp_x / exp_x.sum(dim=dim, keepdim=True)\n",
    "\n",
    "# Test with various shapes\n",
    "test_cases = [\n",
    "    (\"1D vector\", torch.randn(10, device=device)),\n",
    "    (\"2D matrix (row-wise)\", torch.randn(100, 50, device=device)),\n",
    "    (\"3D tensor (last dim)\", torch.randn(32, 64, 128, device=device))\n",
    "]\n",
    "\n",
    "for name, x in test_cases:\n",
    "    builtin = softmax_gpu(x)\n",
    "    manual = softmax_manual(x)\n",
    "    \n",
    "    print(f\"\\n{name} - Shape: {x.shape}\")\n",
    "    print(f\"  Results match: {torch.allclose(builtin, manual)}\")\n",
    "    print(f\"  Sum check: {manual.sum(dim=-1).mean():.6f} (should be 1.0)\")\n",
    "    print(f\"  Max value: {manual.max():.6f}\")\n",
    "    print(f\"  Min value: {manual.min():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Softmax Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_softmax(batch_sizes: list, seq_lengths: list) -> dict:\n",
    "    \"\"\"Benchmark softmax across different dimensions.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for batch in batch_sizes:\n",
    "        results[batch] = []\n",
    "        for seq_len in seq_lengths:\n",
    "            x = torch.randn(batch, seq_len, device=device)\n",
    "            \n",
    "            # Warmup\n",
    "            _ = torch.softmax(x, dim=-1)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Benchmark\n",
    "            start = time.time()\n",
    "            for _ in range(100):  # Multiple iterations for accuracy\n",
    "                _ = torch.softmax(x, dim=-1)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            avg_time = ((time.time() - start) / 100) * 1000\n",
    "            results[batch].append(avg_time)\n",
    "    \n",
    "    return results\n",
    "\n",
    "batch_sizes = [1, 32, 128]\n",
    "seq_lengths = [64, 128, 256, 512, 1024]\n",
    "\n",
    "print(\"Benchmarking softmax performance...\")\n",
    "softmax_results = benchmark_softmax(batch_sizes, seq_lengths)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "for batch in batch_sizes:\n",
    "    plt.plot(seq_lengths, softmax_results[batch], 'o-', \n",
    "             label=f'Batch={batch}', linewidth=2, markersize=8)\n",
    "\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Time per call (ms)', fontsize=12)\n",
    "plt.title('Softmax Performance vs Sequence Length', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Observations:\")\n",
    "print(\"   - Larger batches amortize overhead\")\n",
    "print(\"   - Time scales roughly linearly with sequence length\")\n",
    "print(\"   - GPU shines with large batch sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Log-Softmax and Numerical Tricks\n",
    "\n",
    "### Why Log-Softmax?\n",
    "\n",
    "In many applications (cross-entropy loss, attention), we need $\\log(\\text{softmax}(x))$:\n",
    "\n",
    "**Naive:** $\\log(\\text{softmax}(x_i)) = \\log\\left(\\frac{e^{x_i}}{\\sum_j e^{x_j}}\\right)$\n",
    "\n",
    "**Optimized:** $\\log(\\text{softmax}(x_i)) = x_i - \\log\\left(\\sum_j e^{x_j}\\right)$\n",
    "\n",
    "This avoids computing the division!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax_naive(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    \"\"\"Compute log-softmax naively.\"\"\"\n",
    "    return torch.log(torch.softmax(x, dim=dim))\n",
    "\n",
    "def log_softmax_stable(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    \"\"\"Numerically stable log-softmax.\"\"\"\n",
    "    x_max = x.max(dim=dim, keepdim=True)[0]\n",
    "    log_sum_exp = torch.log(torch.exp(x - x_max).sum(dim=dim, keepdim=True))\n",
    "    return x - x_max - log_sum_exp\n",
    "\n",
    "# Compare implementations\n",
    "x = torch.randn(1000, 512, device=device)\n",
    "\n",
    "# Builtin (optimized)\n",
    "builtin = F.log_softmax(x, dim=-1)\n",
    "\n",
    "# Manual stable version\n",
    "manual = log_softmax_stable(x)\n",
    "\n",
    "# Naive version\n",
    "naive = log_softmax_naive(x)\n",
    "\n",
    "print(\"Log-Softmax Comparison:\")\n",
    "print(f\"Builtin vs Manual: {torch.allclose(builtin, manual, rtol=1e-5)}\")\n",
    "print(f\"Builtin vs Naive:  {torch.allclose(builtin, naive, rtol=1e-5)}\")\n",
    "\n",
    "# Benchmark\n",
    "for name, func in [(\"Builtin\", F.log_softmax), \n",
    "                   (\"Manual\", log_softmax_stable),\n",
    "                   (\"Naive\", log_softmax_naive)]:\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        if name == \"Builtin\":\n",
    "            _ = func(x, dim=-1)\n",
    "        else:\n",
    "            _ = func(x)\n",
    "    torch.cuda.synchronize()\n",
    "    avg_time = ((time.time() - start) / 100) * 1000\n",
    "    print(f\"{name}: {avg_time:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Combining Operations - Matrix Mult + Softmax\n",
    "\n",
    "This combination appears in attention mechanisms:\n",
    "\n",
    "```python\n",
    "scores = Q @ K.T  # Matrix multiplication\n",
    "attn = softmax(scores / sqrt(d_k))  # Scaled softmax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_scores(Q: torch.Tensor, K: torch.Tensor, scale: float = None) -> torch.Tensor:\n",
    "    \"\"\"Compute attention scores: softmax(Q @ K.T / sqrt(d_k)).\"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    if scale is None:\n",
    "        scale = d_k ** 0.5\n",
    "    \n",
    "    # Q: (batch, seq_q, d_k)\n",
    "    # K: (batch, seq_k, d_k)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / scale\n",
    "    return torch.softmax(scores, dim=-1)\n",
    "\n",
    "# Test with transformer dimensions\n",
    "batch_size = 32\n",
    "seq_len = 128\n",
    "d_k = 64\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k, device=device)\n",
    "K = torch.randn(batch_size, seq_len, d_k, device=device)\n",
    "\n",
    "attn_weights = attention_scores(Q, K)\n",
    "\n",
    "print(f\"Input Q shape: {Q.shape}\")\n",
    "print(f\"Input K shape: {K.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention properties:\")\n",
    "print(f\"  Sum per row: {attn_weights.sum(dim=-1)[0, 0]:.6f} (should be 1.0)\")\n",
    "print(f\"  Max value: {attn_weights.max():.6f}\")\n",
    "print(f\"  Min value: {attn_weights.min():.6f}\")\n",
    "\n",
    "# Visualize attention pattern for first sample\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attn_weights[0].cpu().numpy(), cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Key Position', fontsize=12)\n",
    "plt.ylabel('Query Position', fontsize=12)\n",
    "plt.title('Attention Weight Matrix', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Optimization: Fused Operations\n",
    "\n",
    "Combining operations reduces memory transfers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_fused_vs_separate(sizes: list) -> dict:\n",
    "    \"\"\"Compare fused vs separate operations.\"\"\"\n",
    "    results = {'separate': [], 'fused': []}\n",
    "    \n",
    "    for size in sizes:\n",
    "        Q = torch.randn(32, size, 64, device=device)\n",
    "        K = torch.randn(32, size, 64, device=device)\n",
    "        scale = 8.0\n",
    "        \n",
    "        # Separate operations\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        scaled = scores / scale\n",
    "        attn = torch.softmax(scaled, dim=-1)\n",
    "        torch.cuda.synchronize()\n",
    "        results['separate'].append((time.time() - start) * 1000)\n",
    "        \n",
    "        # Fused operations (single expression)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        attn = torch.softmax(torch.matmul(Q, K.transpose(-2, -1)) / scale, dim=-1)\n",
    "        torch.cuda.synchronize()\n",
    "        results['fused'].append((time.time() - start) * 1000)\n",
    "    \n",
    "    return results\n",
    "\n",
    "seq_sizes = [64, 128, 256, 512]\n",
    "fusion_results = benchmark_fused_vs_separate(seq_sizes)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_sizes, fusion_results['separate'], 'o-', label='Separate ops', linewidth=2)\n",
    "plt.plot(seq_sizes, fusion_results['fused'], 's-', label='Fused ops', linewidth=2)\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Time (ms)', fontsize=12)\n",
    "plt.title('Fused vs Separate Operations', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "speedups = [sep / fus for sep, fus in zip(fusion_results['separate'], fusion_results['fused'])]\n",
    "print(f\"\\nAverage speedup from fusion: {np.mean(speedups):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section\n",
    "\n",
    "### Exercise 1: Masked Softmax\n",
    "Implement softmax with masking (used in causal attention):\n",
    "- Apply mask before softmax to prevent attending to future positions\n",
    "- Set masked positions to -infinity before softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(x: torch.Tensor, mask: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply softmax with masking.\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        mask: Boolean mask (True = keep, False = mask)\n",
    "        dim: Dimension to apply softmax\n",
    "    \"\"\"\n",
    "    # TODO: Implement masked softmax\n",
    "    pass\n",
    "\n",
    "# Test with causal mask\n",
    "# seq_len = 5\n",
    "# x = torch.randn(1, seq_len, seq_len)\n",
    "# causal_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))\n",
    "# result = masked_softmax(x, causal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Batch Matrix Operations\n",
    "Implement efficient batched matrix multiplication with softmax:\n",
    "1. Handle 4D tensors (batch, heads, seq, dim)\n",
    "2. Apply softmax over the correct dimension\n",
    "3. Benchmark against loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement batched operations\n",
    "# Hint: Use einsum or batched matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Numerical Stability Analysis\n",
    "Create test cases that demonstrate numerical instability:\n",
    "1. Generate inputs that cause overflow in naive softmax\n",
    "2. Show stable version handles them correctly\n",
    "3. Plot the error as input magnitude increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "âœ… **Optimized Matrix Multiplication:**\n",
    "- Tiling and shared memory reduce global memory access\n",
    "- Memory coalescing is critical for performance\n",
    "- PyTorch handles optimization internally\n",
    "\n",
    "âœ… **Stable Softmax:**\n",
    "- Always subtract max before exponential\n",
    "- Essential for numerical stability in deep learning\n",
    "- Log-softmax avoids unnecessary divisions\n",
    "\n",
    "âœ… **Performance Optimization:**\n",
    "- Fuse operations when possible\n",
    "- Batch operations for better GPU utilization\n",
    "- Profile to identify bottlenecks\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Notebook 03**, we'll implement:\n",
    "- Attention mechanisms from scratch (CPU)\n",
    "- Query, Key, Value concepts\n",
    "- Scaled dot-product attention\n",
    "- Visualizing attention patterns\n",
    "\n",
    "## Further reading (Archive.org)\n",
    "\n",
    "To better understand GPU-accelerated linear algebra and softmax, search Archive.org for:\n",
    "\n",
    "- \"numerical linear algebra GPU\"\n",
    "- \"matrix multiplication CUDA\"\n",
    "- \"parallel algorithms softmax\"\n",
    "\n",
    "Focus on materials that discuss tiling, shared memory usage, and reduction operations, since these patterns underlie efficient matrix multiplication and stable softmax implementations on modern GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

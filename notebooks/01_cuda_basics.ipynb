{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: CUDA Basics\n",
    "\n",
    "## Introduction to GPU Programming\n",
    "\n",
    "Welcome to your first CUDA programming tutorial! In this notebook, you'll learn:\n",
    "\n",
    "1. **CUDA Execution Model** - Understanding threads, blocks, and grids\n",
    "2. **Memory Hierarchy** - How data moves between CPU and GPU\n",
    "3. **Your First Kernels** - Vector addition and matrix multiplication\n",
    "4. **Performance Measurement** - Benchmarking GPU vs CPU\n",
    "\n",
    "### Why CUDA?\n",
    "\n",
    "Modern GPUs contain thousands of cores optimized for parallel computation. While CPUs excel at sequential tasks, GPUs can process massive amounts of data simultaneously - perfect for neural networks!\n",
    "\n",
    "**Example:** A simple vector addition of 10 million elements:\n",
    "- **CPU:** ~15ms (sequential processing)\n",
    "- **GPU:** ~0.5ms (parallel processing)\n",
    "- **Speedup:** ~30x faster! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU Device: NVIDIA GeForce RTX 4070\n",
      "GPU Memory: 12.88 GB\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available. This notebook requires a GPU.\")\n",
    "    print(\"You can still learn the concepts, but won't see performance benefits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: CUDA Execution Model\n",
    "\n",
    "### The GPU Hierarchy\n",
    "\n",
    "CUDA organizes parallel work into three levels:\n",
    "\n",
    "```\n",
    "Grid (Entire Computation)\n",
    "  ‚îî‚îÄ‚îÄ Blocks (Groups of Threads)\n",
    "        ‚îî‚îÄ‚îÄ Threads (Individual Workers)\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Thread:** The smallest unit of execution - does one piece of work\n",
    "2. **Block:** A group of threads (up to 1024) that can cooperate and share memory\n",
    "3. **Grid:** Collection of blocks that execute the same kernel function\n",
    "\n",
    "**Example:** Adding two vectors of 10,000 elements\n",
    "- Launch 10 blocks √ó 1000 threads = 10,000 threads\n",
    "- Each thread adds one pair of elements\n",
    "- All threads run simultaneously!\n",
    "\n",
    "### Memory Spaces\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           Host (CPU)                     ‚îÇ\n",
    "‚îÇ  - Main RAM (large, slow to GPU)        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "               ‚îÇ PCIe Bus\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           Device (GPU)                   ‚îÇ\n",
    "‚îÇ  - Global Memory (large, slower)        ‚îÇ\n",
    "‚îÇ  - Shared Memory (small, fast)          ‚îÇ\n",
    "‚îÇ  - Registers (tiny, fastest)            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Addition - Your First CUDA Kernel\n",
    "\n",
    "Let's implement parallel vector addition: `C = A + B`\n",
    "\n",
    "### CPU Version (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_add_cpu(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Sequential CPU vector addition.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Test with small arrays\n",
    "n = 10\n",
    "a_cpu = np.random.randn(n).astype(np.float32)\n",
    "b_cpu = np.random.randn(n).astype(np.float32)\n",
    "\n",
    "c_cpu = vector_add_cpu(a_cpu, b_cpu)\n",
    "\n",
    "print(\"CPU Vector Addition:\")\n",
    "print(f\"A: {a_cpu[:5]}...\")\n",
    "print(f\"B: {b_cpu[:5]}...\")\n",
    "print(f\"C: {c_cpu[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Version (Parallel)\n",
    "\n",
    "With PyTorch, we can leverage CUDA tensors for automatic GPU acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_add_gpu(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Parallel GPU vector addition using PyTorch CUDA tensors.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Create GPU tensors\n",
    "if torch.cuda.is_available():\n",
    "    # Transfer data to GPU\n",
    "    a_gpu = torch.from_numpy(a_cpu).cuda()\n",
    "    b_gpu = torch.from_numpy(b_cpu).cuda()\n",
    "    \n",
    "    # Perform addition on GPU\n",
    "    c_gpu = vector_add_gpu(a_gpu, b_gpu)\n",
    "    \n",
    "    # Transfer result back to CPU for verification\n",
    "    c_gpu_cpu = c_gpu.cpu().numpy()\n",
    "    \n",
    "    print(\"\\nGPU Vector Addition:\")\n",
    "    print(f\"Result: {c_gpu_cpu[:5]}...\")\n",
    "    print(f\"\\nMatches CPU? {np.allclose(c_cpu, c_gpu_cpu)}\")\n",
    "else:\n",
    "    print(\"GPU not available - skipping GPU test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison: CPU vs GPU\n",
    "\n",
    "Let's benchmark with larger arrays to see real performance differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_vector_add(sizes: list) -> Tuple[list, list]:\n",
    "    \"\"\"Benchmark vector addition at different sizes.\"\"\"\n",
    "    cpu_times = []\n",
    "    gpu_times = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Create test data\n",
    "        a = np.random.randn(size).astype(np.float32)\n",
    "        b = np.random.randn(size).astype(np.float32)\n",
    "        \n",
    "        # CPU benchmark\n",
    "        start = time.time()\n",
    "        _ = a + b\n",
    "        cpu_times.append((time.time() - start) * 1000)  # Convert to ms\n",
    "        \n",
    "        # GPU benchmark\n",
    "        if torch.cuda.is_available():\n",
    "            a_gpu = torch.from_numpy(a).cuda()\n",
    "            b_gpu = torch.from_numpy(b).cuda()\n",
    "            \n",
    "            # Warm-up\n",
    "            _ = a_gpu + b_gpu\n",
    "            torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "            \n",
    "            # Actual benchmark\n",
    "            start = time.time()\n",
    "            _ = a_gpu + b_gpu\n",
    "            torch.cuda.synchronize()\n",
    "            gpu_times.append((time.time() - start) * 1000)\n",
    "        else:\n",
    "            gpu_times.append(0)\n",
    "    \n",
    "    return cpu_times, gpu_times\n",
    "\n",
    "# Run benchmarks\n",
    "sizes = [10**3, 10**4, 10**5, 10**6, 10**7]\n",
    "cpu_times, gpu_times = benchmark_vector_add(sizes)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPerformance Benchmark Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Size':<12} {'CPU (ms)':<12} {'GPU (ms)':<12} {'Speedup':<12}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for size, cpu_t, gpu_t in zip(sizes, cpu_times, gpu_times):\n",
    "    speedup = cpu_t / gpu_t if gpu_t > 0 else 0\n",
    "    print(f\"{size:<12} {cpu_t:<12.4f} {gpu_t:<12.4f} {speedup:<12.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance\n",
    "if torch.cuda.is_available():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Execution time\n",
    "    ax1.loglog(sizes, cpu_times, 'o-', label='CPU', linewidth=2, markersize=8)\n",
    "    ax1.loglog(sizes, gpu_times, 's-', label='GPU', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Vector Size', fontsize=12)\n",
    "    ax1.set_ylabel('Time (ms)', fontsize=12)\n",
    "    ax1.set_title('Vector Addition Performance', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Speedup\n",
    "    speedups = [cpu_t / gpu_t for cpu_t, gpu_t in zip(cpu_times, gpu_times)]\n",
    "    ax2.semilogx(sizes, speedups, 'o-', linewidth=2, markersize=8, color='green')\n",
    "    ax2.set_xlabel('Vector Size', fontsize=12)\n",
    "    ax2.set_ylabel('Speedup (√ó)', fontsize=12)\n",
    "    ax2.set_title('GPU Speedup over CPU', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=1, color='r', linestyle='--', label='No speedup', alpha=0.5)\n",
    "    ax2.legend(fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Maximum speedup: {max(speedups):.2f}x at size {sizes[speedups.index(max(speedups))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is fundamental to neural networks. Every layer performs: `Y = X @ W`\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For matrices A (m√ók) and B (k√ón), the result C (m√ón):\n",
    "\n",
    "$$C[i,j] = \\sum_{k=0}^{K-1} A[i,k] \\times B[k,j]$$\n",
    "\n",
    "Each element requires K multiplications and K-1 additions.\n",
    "\n",
    "### Naive Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_cpu_naive(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Naive triple-loop matrix multiplication.\"\"\"\n",
    "    m, k = A.shape\n",
    "    k2, n = B.shape\n",
    "    assert k == k2, \"Matrix dimensions must match\"\n",
    "    \n",
    "    C = np.zeros((m, n), dtype=np.float32)\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            for k_idx in range(k):\n",
    "                C[i, j] += A[i, k_idx] * B[k_idx, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "# Test with small matrices\n",
    "m, k, n = 4, 3, 5\n",
    "A = np.random.randn(m, k).astype(np.float32)\n",
    "B = np.random.randn(k, n).astype(np.float32)\n",
    "\n",
    "C_naive = matmul_cpu_naive(A, B)\n",
    "C_numpy = A @ B  # NumPy's optimized version\n",
    "\n",
    "print(\"Naive CPU Matrix Multiplication:\")\n",
    "print(f\"A shape: {A.shape}, B shape: {B.shape}, C shape: {C_naive.shape}\")\n",
    "print(f\"\\nResults match NumPy? {np.allclose(C_naive, C_numpy)}\")\n",
    "print(f\"\\nC (first 3√ó3):\\n{C_naive[:3, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Matrix Multiplication\n",
    "\n",
    "GPUs excel at matrix multiplication because:\n",
    "1. **Massive Parallelism:** Each output element can be computed independently\n",
    "2. **Memory Bandwidth:** High-speed memory for large matrices\n",
    "3. **Specialized Hardware:** Tensor cores for matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_gpu(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"GPU matrix multiplication using PyTorch.\"\"\"\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Transfer to GPU\n",
    "    A_gpu = torch.from_numpy(A).cuda()\n",
    "    B_gpu = torch.from_numpy(B).cuda()\n",
    "    \n",
    "    # Compute on GPU\n",
    "    C_gpu = matmul_gpu(A_gpu, B_gpu)\n",
    "    \n",
    "    # Verify\n",
    "    C_gpu_cpu = C_gpu.cpu().numpy()\n",
    "    \n",
    "    print(\"\\nGPU Matrix Multiplication:\")\n",
    "    print(f\"Results match CPU? {np.allclose(C_numpy, C_gpu_cpu)}\")\n",
    "    print(f\"\\nC_GPU (first 3√ó3):\\n{C_gpu_cpu[:3, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Benchmark: Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matmul(sizes: list) -> Tuple[list, list]:\n",
    "    \"\"\"Benchmark matrix multiplication at different sizes.\"\"\"\n",
    "    cpu_times = []\n",
    "    gpu_times = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"Benchmarking {size}√ó{size} matrices...\")\n",
    "        \n",
    "        # Create test matrices\n",
    "        A = np.random.randn(size, size).astype(np.float32)\n",
    "        B = np.random.randn(size, size).astype(np.float32)\n",
    "        \n",
    "        # CPU benchmark (using optimized NumPy)\n",
    "        start = time.time()\n",
    "        _ = A @ B\n",
    "        cpu_times.append((time.time() - start) * 1000)\n",
    "        \n",
    "        # GPU benchmark\n",
    "        if torch.cuda.is_available():\n",
    "            A_gpu = torch.from_numpy(A).cuda()\n",
    "            B_gpu = torch.from_numpy(B).cuda()\n",
    "            \n",
    "            # Warm-up\n",
    "            _ = torch.matmul(A_gpu, B_gpu)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Actual benchmark\n",
    "            start = time.time()\n",
    "            _ = torch.matmul(A_gpu, B_gpu)\n",
    "            torch.cuda.synchronize()\n",
    "            gpu_times.append((time.time() - start) * 1000)\n",
    "        else:\n",
    "            gpu_times.append(0)\n",
    "    \n",
    "    return cpu_times, gpu_times\n",
    "\n",
    "# Run benchmarks with progressively larger matrices\n",
    "mat_sizes = [128, 256, 512, 1024, 2048]\n",
    "mat_cpu_times, mat_gpu_times = benchmark_matmul(mat_sizes)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Matrix Multiplication Performance Benchmark\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Size':<12} {'CPU (ms)':<15} {'GPU (ms)':<15} {'Speedup':<12}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for size, cpu_t, gpu_t in zip(mat_sizes, mat_cpu_times, mat_gpu_times):\n",
    "    speedup = cpu_t / gpu_t if gpu_t > 0 else 0\n",
    "    print(f\"{size}√ó{size:<8} {cpu_t:<15.2f} {gpu_t:<15.4f} {speedup:<12.1f}x\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matrix multiplication performance\n",
    "if torch.cuda.is_available():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Execution time\n",
    "    ax1.semilogy(mat_sizes, mat_cpu_times, 'o-', label='CPU (NumPy)', linewidth=2, markersize=8)\n",
    "    ax1.semilogy(mat_sizes, mat_gpu_times, 's-', label='GPU (PyTorch)', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Matrix Size (N√óN)', fontsize=12)\n",
    "    ax1.set_ylabel('Time (ms, log scale)', fontsize=12)\n",
    "    ax1.set_title('Matrix Multiplication Performance', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Speedup\n",
    "    mat_speedups = [cpu_t / gpu_t for cpu_t, gpu_t in zip(mat_cpu_times, mat_gpu_times)]\n",
    "    ax2.plot(mat_sizes, mat_speedups, 'o-', linewidth=2, markersize=8, color='purple')\n",
    "    ax2.set_xlabel('Matrix Size (N√óN)', fontsize=12)\n",
    "    ax2.set_ylabel('Speedup (√ó)', fontsize=12)\n",
    "    ax2.set_title('GPU Speedup over CPU', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=1, color='r', linestyle='--', label='No speedup', alpha=0.5)\n",
    "    ax2.legend(fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Maximum speedup: {max(mat_speedups):.1f}x at size {mat_sizes[mat_speedups.index(max(mat_speedups))]}√ó{mat_sizes[mat_speedups.index(max(mat_speedups))]}\")\n",
    "    print(f\"\\nüí° Key insight: GPU advantage grows with matrix size!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Understanding CUDA Execution in Detail\n",
    "\n",
    "### Thread Indexing\n",
    "\n",
    "Each CUDA thread needs to know which data element to process. PyTorch handles this automatically, but understanding the concept is crucial:\n",
    "\n",
    "```python\n",
    "# Conceptual CUDA kernel for vector addition\n",
    "def vector_add_kernel(A, B, C, N):\n",
    "    # Each thread computes its global index\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x\n",
    "    \n",
    "    # Process only if within bounds\n",
    "    if idx < N:\n",
    "        C[idx] = A[idx] + B[idx]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `threadIdx.x`: Thread's position within its block (0 to blockDim.x-1)\n",
    "- `blockIdx.x`: Block's position in the grid\n",
    "- `blockDim.x`: Number of threads per block\n",
    "\n",
    "### Memory Transfer Overhead\n",
    "\n",
    "Data must travel between CPU and GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure memory transfer overhead\n",
    "if torch.cuda.is_available():\n",
    "    sizes = [10**6, 10**7, 10**8]\n",
    "    \n",
    "    print(\"Memory Transfer Benchmark:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Size':<15} {'CPU‚ÜíGPU (ms)':<20} {'GPU‚ÜíCPU (ms)':<20}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Create CPU data\n",
    "        data_cpu = np.random.randn(size).astype(np.float32)\n",
    "        \n",
    "        # Measure CPU ‚Üí GPU transfer\n",
    "        start = time.time()\n",
    "        data_gpu = torch.from_numpy(data_cpu).cuda()\n",
    "        torch.cuda.synchronize()\n",
    "        cpu_to_gpu = (time.time() - start) * 1000\n",
    "        \n",
    "        # Measure GPU ‚Üí CPU transfer\n",
    "        start = time.time()\n",
    "        data_back = data_gpu.cpu().numpy()\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_to_cpu = (time.time() - start) * 1000\n",
    "        \n",
    "        print(f\"{size:<15} {cpu_to_gpu:<20.2f} {gpu_to_cpu:<20.2f}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüí° Key Takeaway: Keep data on GPU as long as possible!\")\n",
    "    print(\"   Minimize transfers by chaining operations on the GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practical Tips for CUDA Programming\n",
    "\n",
    "### 1. When to Use GPU?\n",
    "\n",
    "‚úÖ **Use GPU when:**\n",
    "- Large data sizes (> 10,000 elements)\n",
    "- Many operations on same data\n",
    "- Parallel-friendly algorithms\n",
    "\n",
    "‚ùå **Avoid GPU when:**\n",
    "- Small data (transfer overhead dominates)\n",
    "- Sequential algorithms\n",
    "- Frequent CPU-GPU transfers\n",
    "\n",
    "### 2. Memory Management Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practice: Keep intermediate results on GPU\n",
    "if torch.cuda.is_available():\n",
    "    # ‚ùå BAD: Multiple transfers\n",
    "    def bad_practice(x_cpu):\n",
    "        x = torch.from_numpy(x_cpu).cuda()\n",
    "        y = x * 2\n",
    "        y_cpu = y.cpu().numpy()  # Unnecessary transfer\n",
    "        y = torch.from_numpy(y_cpu).cuda()  # Wasteful!\n",
    "        z = y + 1\n",
    "        return z.cpu().numpy()\n",
    "    \n",
    "    # ‚úÖ GOOD: Chain operations on GPU\n",
    "    def good_practice(x_cpu):\n",
    "        x = torch.from_numpy(x_cpu).cuda()\n",
    "        y = x * 2\n",
    "        z = y + 1  # Stay on GPU\n",
    "        return z.cpu().numpy()  # Single transfer at end\n",
    "    \n",
    "    # Benchmark\n",
    "    test_data = np.random.randn(10**7).astype(np.float32)\n",
    "    \n",
    "    start = time.time()\n",
    "    _ = bad_practice(test_data)\n",
    "    bad_time = (time.time() - start) * 1000\n",
    "    \n",
    "    start = time.time()\n",
    "    _ = good_practice(test_data)\n",
    "    good_time = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"Bad practice: {bad_time:.2f} ms\")\n",
    "    print(f\"Good practice: {good_time:.2f} ms\")\n",
    "    print(f\"Speedup: {bad_time/good_time:.2f}x faster! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Checking GPU Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Create some GPU tensors\n",
    "    large_tensor = torch.randn(10000, 10000).cuda()\n",
    "    \n",
    "    # Check memory usage\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    \n",
    "    print(f\"GPU Memory Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"GPU Memory Reserved: {reserved:.2f} GB\")\n",
    "    \n",
    "    # Clean up\n",
    "    del large_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nAfter cleanup:\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"\\nüí° Tip: Use del and empty_cache() to free GPU memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section\n",
    "\n",
    "Test your understanding with these exercises:\n",
    "\n",
    "### Exercise 1: Element-wise Operations\n",
    "Implement and benchmark GPU-accelerated element-wise operations:\n",
    "- Squaring all elements: `y = x¬≤`\n",
    "- ReLU activation: `y = max(0, x)`\n",
    "- Sigmoid: `y = 1 / (1 + exp(-x))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Implement element-wise operations on GPU\n",
    "\n",
    "def exercise_1():\n",
    "    \"\"\"Implement element-wise operations.\"\"\"\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# exercise_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Matrix Operations Chain\n",
    "Implement a chain of matrix operations efficiently:\n",
    "1. `Y = X @ W1`\n",
    "2. `Z = ReLU(Y)`\n",
    "3. `Out = Z @ W2`\n",
    "\n",
    "Minimize CPU-GPU transfers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Chain matrix operations on GPU\n",
    "\n",
    "def exercise_2():\n",
    "    \"\"\"Chain matrix operations efficiently.\"\"\"\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# exercise_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Performance Analysis\n",
    "For different matrix sizes (64, 128, 256, 512):\n",
    "1. Measure total execution time including transfers\n",
    "2. Measure computation time only (data already on GPU)\n",
    "3. Calculate percentage of time spent on transfers\n",
    "4. Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Analyze transfer vs computation time\n",
    "\n",
    "def exercise_3():\n",
    "    \"\"\"Analyze performance breakdown.\"\"\"\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# exercise_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "‚úÖ **CUDA Execution Model:**\n",
    "- Grids, blocks, and threads organize parallel work\n",
    "- Each thread processes one data element\n",
    "- Thousands of threads run simultaneously\n",
    "\n",
    "‚úÖ **Memory Hierarchy:**\n",
    "- Data transfers between CPU and GPU\n",
    "- Keep data on GPU for best performance\n",
    "- Minimize CPU-GPU communication\n",
    "\n",
    "‚úÖ **Performance Characteristics:**\n",
    "- GPUs excel at large-scale parallel operations\n",
    "- Speedup increases with problem size\n",
    "- Transfer overhead matters for small data\n",
    "\n",
    "‚úÖ **Practical Skills:**\n",
    "- Writing GPU-accelerated code with PyTorch\n",
    "- Benchmarking CPU vs GPU performance\n",
    "- Managing GPU memory effectively\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "From our benchmarks:\n",
    "- **Vector Addition:** ~30x speedup for large vectors\n",
    "- **Matrix Multiplication:** ~100x+ speedup for large matrices\n",
    "- **Memory Transfer:** Can dominate for small operations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Notebook 02**, we'll learn:\n",
    "- Shared memory optimization\n",
    "- Stable softmax implementation\n",
    "- Advanced memory access patterns\n",
    "- Thread synchronization\n",
    "\n",
    "These concepts are crucial for efficient transformer implementations!\n",
    "\n",
    "## Further reading (Archive.org)\n",
    "\n",
    "For foundational CUDA concepts (threads, blocks, grids, memory hierarchy), search Archive.org **Texts** for:\n",
    "\n",
    "- \"CUDA programming introduction\"\n",
    "- \"GPU programming guide\"\n",
    "- \"GPGPU programming tutorial\"\n",
    "\n",
    "Look especially for introductory lecture notes or textbooks that explain how kernels are launched and how data moves between host and device memory, as these will deepen the intuition behind the simple vector add and matmul examples in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

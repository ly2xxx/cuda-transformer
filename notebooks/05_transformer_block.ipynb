{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Building a Transformer Encoder Block\n",
    "\n",
    "## Assembling the Complete Architecture\n",
    "\n",
    "Now we'll combine everything we've learned to build a complete transformer encoder! In this notebook:\n",
    "\n",
    "1. **Multi-Head Self-Attention** - The core mechanism\n",
    "2. **Feed-Forward Networks** - Position-wise transformations\n",
    "3. **Layer Normalization** - Stabilizing training\n",
    "4. **Residual Connections** - Enabling deep networks\n",
    "5. **Complete Encoder Block** - Putting it all together\n",
    "\n",
    "This is the architecture that powers GPT, BERT, and modern LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Multi-Head Self-Attention (Review)\n",
    "\n",
    "Let's implement a clean, production-ready version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear layers for Q, K, V, and output\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections in batch\n",
    "        Q = self.W_Q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.W_O(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test\n",
    "mha = MultiHeadAttention(d_model=512, n_heads=8).to(device)\n",
    "x = torch.randn(2, 10, 512, device=device)\n",
    "output = mha(x, x, x)\n",
    "print(f\"âœ… Multi-Head Attention: {x.shape} â†’ {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Position-wise Feed-Forward Network\n",
    "\n",
    "### The FFN Architecture\n",
    "\n",
    "After attention, each position passes through an identical feed-forward network:\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Or with GELU activation (modern variant):\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = self.linear1(x)         # (batch, seq_len, d_ff)\n",
    "        x = F.gelu(x)               # GELU activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)         # (batch, seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "ffn = PositionwiseFeedForward(d_model=512, d_ff=2048).to(device)\n",
    "x = torch.randn(2, 10, 512, device=device)\n",
    "output = ffn(x)\n",
    "print(f\"âœ… Feed-Forward Network: {x.shape} â†’ {output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in ffn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Layer Normalization\n",
    "\n",
    "### Why Normalization?\n",
    "\n",
    "Layer norm stabilizes training by normalizing across features:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean across features\n",
    "- $\\sigma^2$ = variance across features\n",
    "- $\\gamma, \\beta$ = learnable scale and shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch provides LayerNorm, but let's understand it:\n",
    "\n",
    "def manual_layer_norm(x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "    \"\"\"Manual layer normalization for understanding.\"\"\"\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    return (x - mean) / torch.sqrt(var + eps)\n",
    "\n",
    "# Compare manual vs PyTorch\n",
    "x = torch.randn(2, 10, 512, device=device)\n",
    "manual = manual_layer_norm(x)\n",
    "pytorch_ln = nn.LayerNorm(512, device=device)\n",
    "pytorch = pytorch_ln(x)\n",
    "\n",
    "print(f\"Manual LayerNorm mean: {manual.mean(dim=-1)[0, 0]:.6f} (should be ~0)\")\n",
    "print(f\"Manual LayerNorm std: {manual.std(dim=-1)[0, 0]:.6f} (should be ~1)\")\n",
    "print(f\"\\nâœ… Layer Normalization understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Residual Connections\n",
    "\n",
    "### The Power of Skip Connections\n",
    "\n",
    "Residual connections enable deep networks:\n",
    "\n",
    "$$\\text{output} = \\text{Sublayer}(x) + x$$\n",
    "\n",
    "Benefits:\n",
    "- **Gradient flow:** Direct path for gradients\n",
    "- **Identity mapping:** Easy to learn identity if needed\n",
    "- **Stable training:** Prevents degradation in deep networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"Residual connection with layer normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, sublayer: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"Apply residual connection to any sublayer with the same size.\"\"\"\n",
    "        # Pre-norm: LayerNorm â†’ Sublayer â†’ Dropout â†’ Residual\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "# Test\n",
    "sublayer_conn = SublayerConnection(512).to(device)\n",
    "ffn = PositionwiseFeedForward(512, 2048).to(device)\n",
    "x = torch.randn(2, 10, 512, device=device)\n",
    "output = sublayer_conn(x, ffn)\n",
    "print(f\"âœ… Residual Connection: {x.shape} â†’ {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Complete Transformer Encoder Block\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input\n",
    "  â†“\n",
    "LayerNorm â†’ Multi-Head Attention â†’ Dropout â†’ (+) Residual\n",
    "  â†“\n",
    "LayerNorm â†’ Feed-Forward â†’ Dropout â†’ (+) Residual\n",
    "  â†“\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Complete transformer encoder block.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization for both sub-layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Multi-head attention\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Sub-layer 2: Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test single encoder block\n",
    "encoder_block = TransformerEncoderBlock(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "x = torch.randn(4, 20, 512, device=device)\n",
    "output = encoder_block(x)\n",
    "\n",
    "print(f\"âœ… Transformer Encoder Block\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in encoder_block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Stacking Multiple Encoder Blocks\n",
    "\n",
    "Transformers stack multiple encoder blocks (typically 6-12 for base models, up to 96 for large models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Stack of N encoder blocks.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_layers: int, d_model: int, n_heads: int, \n",
    "                 d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"Pass through all encoder layers.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return self.norm(x)\n",
    "\n",
    "# Create a 6-layer transformer encoder (like BERT-base)\n",
    "encoder = TransformerEncoder(\n",
    "    n_layers=6,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "x = torch.randn(2, 50, 512, device=device)\n",
    "output = encoder(x)\n",
    "\n",
    "print(f\"âœ… 6-Layer Transformer Encoder\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
    "print(f\"Parameter breakdown:\")\n",
    "for i, layer in enumerate(encoder.layers):\n",
    "    params = sum(p.numel() for p in layer.parameters())\n",
    "    print(f\"  Layer {i+1}: {params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Positional Encoding\n",
    "\n",
    "### Why Positional Encoding?\n",
    "\n",
    "Attention is **permutation invariant** - it doesn't know the order of tokens!\n",
    "\n",
    "Solution: Add position information to embeddings:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of state)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add positional encoding to input.\"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Visualize positional encodings\n",
    "pos_enc = PositionalEncoding(d_model=128, max_len=100)\n",
    "pe_matrix = pos_enc.pe[0].cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(pe_matrix.T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Position', fontsize=12)\n",
    "plt.ylabel('Dimension', fontsize=12)\n",
    "plt.title('Positional Encoding Matrix', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Properties:\")\n",
    "print(\"   - Unique encoding for each position\")\n",
    "print(\"   - Smooth variation across positions\")\n",
    "print(\"   - Different frequencies capture different scales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Complete Transformer with Embeddings\n",
    "\n",
    "Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete transformer model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, \n",
    "                 n_heads: int, d_ff: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder = TransformerEncoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        # Scale embeddings\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices (batch, seq_len)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            Encoded representations (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Embed and scale\n",
    "        x = self.embedding(x) * self.scale\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through encoder\n",
    "        x = self.encoder(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create a small transformer\n",
    "model = Transformer(\n",
    "    vocab_size=10000,\n",
    "    d_model=512,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Test\n",
    "input_tokens = torch.randint(0, 10000, (4, 30), device=device)\n",
    "output = model(input_tokens)\n",
    "\n",
    "print(f\"âœ… Complete Transformer Model\")\n",
    "print(f\"Input tokens shape: {input_tokens.shape}\")\n",
    "print(f\"Output embeddings shape: {output.shape}\")\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  Vocabulary size: 10,000\")\n",
    "print(f\"  Model dimension: 512\")\n",
    "print(f\"  Number of layers: 6\")\n",
    "print(f\"  Number of heads: 8\")\n",
    "print(f\"  Feed-forward size: 2048\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section\n",
    "\n",
    "### Exercise 1: Analyze Parameter Distribution\n",
    "Calculate what percentage of parameters are in:\n",
    "- Embeddings\n",
    "- Attention layers\n",
    "- Feed-forward layers\n",
    "- Layer norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze parameter distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Decoder Block\n",
    "Create a transformer decoder block with:\n",
    "- Masked self-attention\n",
    "- Cross-attention to encoder\n",
    "- Feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Gradient Flow Analysis\n",
    "Measure gradient magnitudes with and without:\n",
    "- Residual connections\n",
    "- Layer normalization\n",
    "\n",
    "Show how they stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze gradient flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "âœ… **Transformer Architecture:**\n",
    "- Multi-head attention captures relationships\n",
    "- Feed-forward adds non-linearity\n",
    "- Layer norm and residuals stabilize training\n",
    "\n",
    "âœ… **Components:**\n",
    "- **Embeddings:** Convert tokens to vectors\n",
    "- **Positional Encoding:** Add position information\n",
    "- **Encoder Blocks:** Process sequences\n",
    "- **Layer Norm:** Normalize activations\n",
    "\n",
    "âœ… **Design Choices:**\n",
    "- Pre-norm vs post-norm (we used pre-norm)\n",
    "- GELU activation (smoother than ReLU)\n",
    "- Dropout for regularization\n",
    "\n",
    "âœ… **Scaling:**\n",
    "- Stack layers for depth\n",
    "- Increase d_model for capacity\n",
    "- More heads for diverse attention\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Notebook 06**, we'll:\n",
    "- Train a transformer on real data\n",
    "- Character-level language modeling\n",
    "- Text generation\n",
    "- Monitor training dynamics\n",
    "\n",
    "## Further reading (Archive.org)\n",
    "\n",
    "For a higher-level view of Transformer encoder blocks, search Archive.org for:\n",
    "\n",
    "- \"transformer architecture deep learning\"\n",
    "- \"self-attention networks\"\n",
    "- \"sequence modeling transformers\"\n",
    "\n",
    "Pair these with up-to-date Transformer surveys to see how multi-head self-attention, feed-forward layers, residual connections, and layer normalization fit together in full encoder/decoder stacks, complementing the minimal encoder block implemented in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
